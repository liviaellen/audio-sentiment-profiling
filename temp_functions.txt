def update_rizz_score(emotions: list):
    """
    Update rizz score based on detected emotions.
    Positive emotions add points, negative emotions subtract.
    Score ranges from 0 to 100 (starts at 75).

    Args:
        emotions: List of emotion dicts with 'name' and 'score'
    """
    global audio_stats

    # Calculate adjustment based on top 3 emotions
    adjustment = 0
    for emotion in emotions:
        emotion_name = emotion['name']
        emotion_score = emotion['score']  # 0-1 scale

        if emotion_name in POSITIVE_EMOTIONS:
            # Positive emotions add to rizz (scaled by emotion intensity)
            adjustment += emotion_score * 10  # Max +10 per emotion
        elif emotion_name in NEGATIVE_EMOTIONS:
            # Negative emotions subtract from rizz
            adjustment -= emotion_score * 10  # Max -10 per emotion
        # Neutral emotions don't change the score

    # Update the score and clamp to 0 to 100 range
    audio_stats["rizz_score"] = max(0, min(100, audio_stats["rizz_score"] + adjustment))


def get_rizz_status_text(score: float) -> str:
    """
    Get the rizz status text based on score.

    Args:
        score: Rizz score from 0 to 100

    Returns:
        Status text with emoji
    """
    if score > 80:
        return "ðŸ˜Ž Positive Vibes!"
    elif score >= 40:
        return "ðŸ˜ Neutral Energy"
    else:
        return "ðŸ˜” Negative Energy"


def get_rizz_notification_message(score: float, emotions: list) -> str:
    """
    Generate a short notification message with rizz status, inspirational message, and emotions.
    Format: Rizz score + Inspirational message + Emotions

    Args:
        score: Rizz score from 0 to 100
        emotions: List of emotion names (top 3)

    Returns:
        Formatted notification message
    """
    import random

    emotion_str = ", ".join(emotions)
    score_text = f"{score:.0f}%"

    # Low rizz (< 40) - motivational messages
    if score < 40:
        messages = [
            "Level up bro! ðŸ’ª",
            "Time to bounce back! ðŸš€",
            "Keep your head up! ðŸ’¯",
            "You got this! ðŸ”¥",
            "Comeback mode! âš¡"
        ]
        inspirational = random.choice(messages)
        return f"âš¡ Rizz: {score_text} | {inspirational} | {emotion_str}"

    # Medium rizz (40-80) - neutral messages
    elif score <= 80:
        messages = [
            "Stay balanced! ðŸŽ¯",
            "Keep going! ðŸ’«",
            "Stay steady! ðŸŒŠ",
            "Keep vibing! âœ¨",
            "Stay cool! ðŸ˜Œ"
        ]
        inspirational = random.choice(messages)
        return f"âš¡ Rizz: {score_text} | {inspirational} | {emotion_str}"

    # High rizz (> 80) - positive messages
    else:
        messages = [
            "Killing it! ðŸ”¥",
            "You're on fire! âš¡",
            "Peak vibes! ðŸ’¯",
            "Keep it up! ðŸš€",
            "Boss mode! ðŸ˜Ž"
        ]
        inspirational = random.choice(messages)
        return f"âš¡ Rizz: {score_text} | {inspirational} | {emotion_str}"


# Load emotion notification configuration
def load_emotion_config():
    """Load emotion notification configuration from file or environment"""
    config_file = Path("emotion_config.json")

    # Default configuration - empty thresholds = notify for ALL emotions
    default_config = {
        "notification_enabled": True,
        "emotion_thresholds": {}
    }

    # Try to load from file
    if config_file.exists():
        try:
            with open(config_file, 'r') as f:
                config = json.load(f)
                print(f"âœ“ Loaded emotion config: {config.get('emotion_thresholds')}")
                return config
        except Exception as e:
            print(f"Warning: Could not load emotion_config.json: {e}")
            return default_config

    # Try to load from environment variable
    env_config = os.getenv('EMOTION_NOTIFICATION_CONFIG')
    if env_config:
        try:
            config = json.loads(env_config)
            print(f"âœ“ Loaded emotion config from env: {config.get('emotion_thresholds')}")
            return config
        except Exception as e:
            print(f"Warning: Could not parse EMOTION_NOTIFICATION_CONFIG: {e}")

    print(f"â„¹ï¸  Using default emotion config: {default_config['emotion_thresholds']}")
    return default_config

# Load configuration at startup
EMOTION_CONFIG = load_emotion_config()


def create_wav_header(sample_rate: int, data_size: int) -> bytes:
    """
    Create a WAV file header for the audio data.

    Args:
        sample_rate: Audio sample rate in Hz (typically 8000 or 16000)
        data_size: Size of the audio data in bytes

    Returns:
        44-byte WAV header
    """
    num_channels = 1  # Mono
    bits_per_sample = 16
    byte_rate = sample_rate * num_channels * bits_per_sample // 8
    block_align = num_channels * bits_per_sample // 8

    # RIFF header
    header = bytearray()
    header.extend(b'RIFF')
    header.extend((36 + data_size).to_bytes(4, 'little'))
    header.extend(b'WAVE')

    # fmt subchunk
    header.extend(b'fmt ')
    header.extend((16).to_bytes(4, 'little'))  # Subchunk size
    header.extend((1).to_bytes(2, 'little'))   # Audio format (PCM)
    header.extend(num_channels.to_bytes(2, 'little'))
    header.extend(sample_rate.to_bytes(4, 'little'))
    header.extend(byte_rate.to_bytes(4, 'little'))
    header.extend(block_align.to_bytes(2, 'little'))
    header.extend(bits_per_sample.to_bytes(2, 'little'))

    # data subchunk
    header.extend(b'data')
    header.extend(data_size.to_bytes(4, 'little'))

    return bytes(header)


def upload_to_gcs(file_path: str, bucket_name: str, destination_blob_name: str) -> str:
    """
    Upload a file to Google Cloud Storage.

    Args:
        file_path: Path to the local file
        bucket_name: GCS bucket name
        destination_blob_name: Name for the blob in GCS

    Returns:
        Public URL of the uploaded file
    """
    # Get credentials from environment variable
    credentials_json = os.getenv('GOOGLE_APPLICATION_CREDENTIALS_JSON')
    if not credentials_json:
        raise ValueError("GOOGLE_APPLICATION_CREDENTIALS_JSON environment variable not set")

    # Decode base64 credentials
    try:
        credentials_dict = json.loads(base64.b64decode(credentials_json))
        credentials = service_account.Credentials.from_service_account_info(credentials_dict)
    except Exception as e:
        raise ValueError(f"Failed to decode credentials: {e}")

    # Create GCS client
    client = storage.Client(credentials=credentials, project=credentials_dict.get('project_id'))
    bucket = client.bucket(bucket_name)
    blob = bucket.blob(destination_blob_name)

    # Upload file
    blob.upload_from_filename(file_path, content_type='audio/wav')

    return f"gs://{bucket_name}/{destination_blob_name}"


async def send_omi_notification(
    uid: str,
    message: str
) -> Dict[str, Any]:
    """
    Send a direct notification to Omi user.

    Args:
        uid: Omi user ID
        message: The notification message

    Returns:
        Dict with success status and response
    """
    omi_app_id = os.getenv('OMI_APP_ID')
    omi_api_key = os.getenv('OMI_API_KEY')

    if not omi_app_id or not omi_api_key:
        return {
            "success": False,
            "error": "OMI_APP_ID or OMI_API_KEY not configured"
        }

    try:
        import httpx
        from urllib.parse import quote

        # Make API request to Omi notification endpoint
        url = f"https://api.omi.me/v2/integrations/{omi_app_id}/notification?uid={quote(uid)}&message={quote(message)}"
        headers = {
            "Authorization": f"Bearer {omi_api_key}",
            "Content-Type": "application/json",
            "Content-Length": "0"
        }

        async with httpx.AsyncClient() as client:
            response = await client.post(url, headers=headers, timeout=30.0)

        if response.status_code >= 200 and response.status_code < 300:
            print(f"âœ“ Sent Omi notification to user {uid}")

            # Track notification in stats
            from datetime import datetime, timezone
            notification_data = {
                "timestamp": datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S UTC"),
                "uid": uid,
                "message": message
            }
            audio_stats["recent_notifications"].insert(0, notification_data)
            # Keep only last 10 notifications
            audio_stats["recent_notifications"] = audio_stats["recent_notifications"][:10]

            return {
                "success": True,
                "message": "Notification sent to Omi"
            }
        else:
            error_msg = f"Omi API error: {response.status_code} - {response.text}"
            print(f"âŒ {error_msg}")
            return {
                "success": False,
                "error": error_msg
            }

    except Exception as e:
        print(f"Error sending Omi notification: {e}")
        import traceback
        traceback.print_exc()
        return {
            "success": False,
            "error": str(e)
        }


async def create_omi_memory(
    uid: str,
    text: str,
    emotions: list,
    timestamp: Optional[str] = None
) -> Dict[str, Any]:
    """
    Create a memory in Omi based on detected emotions.

    Args:
        uid: Omi user ID
        text: The emotion summary text to save
        emotions: List of detected emotions
        timestamp: Optional timestamp

    Returns:
        Dict with success status and response
    """
    omi_app_id = os.getenv('OMI_APP_ID')
    omi_api_key = os.getenv('OMI_API_KEY')

    if not omi_app_id or not omi_api_key:
        return {
            "success": False,
            "error": "OMI_APP_ID or OMI_API_KEY not configured"
        }

    try:
        import httpx
        from datetime import datetime, timezone

        # Format emotions into a readable string
        emotion_list = ", ".join([f"{e['name']} ({e['score']:.2f})" for e in emotions[:3]])

        # Create memory data
        memory_data = {
            "memories": [
                {
                    "content": f"Emotion detected: {emotion_list}",
                    "tags": ["emotion", "audio_analysis", emotions[0]['name'].lower()]
                }
            ],
            "text": text,
            "text_source": "other",
            "text_source_spec": "emotion_ai_analysis"
        }

        # Make API request to Omi
        url = f"https://api.omi.me/v2/integrations/{omi_app_id}/user/memories?uid={uid}"
        headers = {
            "Authorization": f"Bearer {omi_api_key}",
            "Content-Type": "application/json"
        }

        async with httpx.AsyncClient() as client:
            response = await client.post(url, json=memory_data, headers=headers, timeout=30.0)

        if response.status_code == 200:
            print(f"âœ“ Created Omi memory for user {uid}")
            return {
                "success": True,
                "message": "Memory created in Omi"
            }
        else:
            error_msg = f"Omi API error: {response.status_code} - {response.text}"
            print(f"âŒ {error_msg}")
            return {
                "success": False,
                "error": error_msg
            }

    except Exception as e:
        print(f"Error creating Omi memory: {e}")
        import traceback
        traceback.print_exc()
        return {
            "success": False,
            "error": str(e)
        }


def generate_emotion_summary() -> Dict[str, Any]:
    """
    Generate a summary of top 5 emotions from statistics.

    Returns:
        Dict with summary text and top emotions
    """
    if not audio_stats["emotion_counts"]:
        return {
            "success": False,
            "error": "No emotion data available"
        }

    # Get top 5 emotions
    sorted_emotions = sorted(
        audio_stats["emotion_counts"].items(),
        key=lambda x: x[1],
        reverse=True
    )[:5]

    total_count = sum(audio_stats["emotion_counts"].values())

    # Build summary text
    emotion_list = []
    emotions_data = []

    for emotion, count in sorted_emotions:
        percentage = (count / total_count * 100)
        emotion_list.append(f"{emotion} ({percentage:.1f}%)")
        emotions_data.append({
            "name": emotion,
            "score": percentage / 100,  # Convert to 0-1 scale
            "count": count
        })

    summary_text = f"ðŸ“Š Emotion Summary - Top 5 emotions detected: {', '.join(emotion_list)}"

    return {
        "success": True,
        "summary": summary_text,
        "emotions": emotions_data,
        "total_detections": total_count
    }


async def save_emotion_memory(uid: Optional[str] = None):
    """
    Save current emotion statistics to Omi memories.

    Args:
        uid: User ID to save memory for. If None, uses last active user.
    """
    # Use last active user if no uid provided
    target_uid = uid or audio_stats.get("last_uid")

    if not target_uid:
        print("âš ï¸ No user ID available for emotion memory")
        return {
            "success": False,
            "error": "No user ID available"
        }

    # Generate emotion summary
    summary = generate_emotion_summary()

    if not summary["success"]:
        print(f"âš ï¸ Cannot create memory: {summary['error']}")
        return summary

    # Create memory in Omi
    result = await create_omi_memory(
        uid=target_uid,
        text=summary["summary"],
        emotions=summary["emotions"]
    )

    return result


async def emotion_memory_background_task():
    """Background task that saves emotion summaries every hour"""
    import asyncio

    while True:
        try:
            # Wait 1 hour (3600 seconds)
            await asyncio.sleep(3600)

            print("â° Running hourly emotion memory save...")
            result = await save_emotion_memory()

            if result.get("success"):
                print("âœ“ Hourly emotion memory saved successfully")
            else:
                print(f"âš ï¸ Hourly emotion memory save failed: {result.get('error')}")

        except Exception as e:
            print(f"Error in emotion memory background task: {e}")
            import traceback
            traceback.print_exc()


async def cleanup_old_audio_files():
    """Background task that deletes audio files older than 5 minutes"""
    import asyncio
    import time

    while True:
        try:
            # Run cleanup every 1 minute
            await asyncio.sleep(60)

            audio_dir = Path("audio_files")
            if not audio_dir.exists():
                continue

            current_time = time.time()
            deleted_count = 0

            # Check all wav files in audio_files directory
            for audio_file in audio_dir.glob("*.wav"):
                try:
                    # Get file age in seconds
                    file_age = current_time - audio_file.stat().st_mtime

                    # Delete if older than 5 minutes (300 seconds)
                    if file_age > 300:
                        audio_file.unlink()
                        deleted_count += 1
                        print(f"ðŸ—‘ï¸  Deleted old audio file: {audio_file.name} (age: {file_age/60:.1f} minutes)")

                except Exception as e:
                    print(f"Warning: Could not delete audio file {audio_file}: {e}")

            if deleted_count > 0:
                print(f"âœ“ Cleanup complete: Deleted {deleted_count} audio file(s)")

        except Exception as e:
            print(f"Error in audio cleanup background task: {e}")
            import traceback
            traceback.print_exc()


def check_emotion_triggers(
    predictions: list,
    emotion_filters: Optional[Dict[str, float]] = None
) -> Dict[str, Any]:
    """
    Check if any emotions meet the threshold criteria.

    Args:
        predictions: List of emotion predictions from Hume
        emotion_filters: Dict of {emotion_name: threshold} to filter by
                        Example: {"Anger": 0.7, "Sadness": 0.6}
                        If None, returns all emotions

    Returns:
        Dict with triggered emotions and details
    """
    triggered_emotions = []

    for prediction in predictions:
        emotions = prediction.get('emotions', [])

        for emotion in emotions:
            emotion_name = emotion['name']
            emotion_score = emotion['score']

            # If no filters, include all
            if not emotion_filters:
                triggered_emotions.append({
                    "name": emotion_name,
                    "score": emotion_score,
                    "time": prediction.get('time')
                })
                continue

            # Check if this emotion is in our filter list (no threshold checking!)
            if emotion_name in emotion_filters:
                triggered_emotions.append({
                    "name": emotion_name,
                    "score": emotion_score,
                    "time": prediction.get('time')
                })

    return {
        "triggered": len(triggered_emotions) > 0,
        "emotions": triggered_emotions,
        "total_triggers": len(triggered_emotions)
    }


async def analyze_text_with_hume(text: str) -> Dict[str, Any]:
    """
    Analyze text with Hume AI Language model for emotional content.

    This analyzes emotion from the text content itself (word choice, phrasing, etc.)
    Different from prosody which analyzes speech tone/pitch.

    Args:
        text: The text to analyze (e.g., transcription from speech-to-text)

    Returns:
        Dict containing emotion predictions for the text
    """
    hume_api_key = os.getenv('HUME_API_KEY')
    if not hume_api_key:
        raise ValueError("HUME_API_KEY environment variable not set")

    try:
        from hume.expression_measurement.stream.stream.types import StreamLanguage

        client = AsyncHumeClient(api_key=hume_api_key)
        # Config with language model for text emotion
        model_config = Config(language=StreamLanguage())

        async with client.expression_measurement.stream.connect() as socket:
            result = await socket.send_text(text, config=model_config)

            # Debug output
            print(f"Text analysis result type: {type(result)}")

            # Check for errors
            if hasattr(result, 'error'):
                return {
                    "success": False,
                    "error": f"Hume API error: {result.error}",
                    "predictions": []
                }

            # Extract language predictions
            if result and hasattr(result, 'language') and result.language:
                lang_preds = result.language.predictions
                print(f"âœ“ Got {len(lang_preds)} text emotion predictions")

                predictions = []
                for prediction in lang_preds:
                    # Sort emotions by score (highest first)
                    sorted_emotions = sorted(
                        prediction.emotions,
                        key=lambda e: e.score,
                        reverse=True
                    )

                    pred_data = {
                        "text": prediction.text if hasattr(prediction, 'text') else None,
                        "position": {
                            "begin": prediction.position.begin if hasattr(prediction, 'position') else None,
                            "end": prediction.position.end if hasattr(prediction, 'position') else None
                        },
                        "emotions": [
                            {"name": emotion.name, "score": emotion.score}
                            for emotion in sorted_emotions
                        ],
                        "top_3_emotions": [
                            {"name": emotion.name, "score": emotion.score}
                            for emotion in sorted_emotions[:3]
                        ]
                    }
                    predictions.append(pred_data)

                return {
                    "success": True,
                    "predictions": predictions,
                    "total_predictions": len(predictions),
                    "analyzed_text": text
                }
            else:
                error_msg = "No language predictions returned from Hume API"
                print(f"âŒ {error_msg}")
                return {
                    "success": False,
                    "error": error_msg,
                    "predictions": []
                }

    except Exception as e:
        print(f"Error analyzing text with Hume: {e}")
        import traceback
        traceback.print_exc()
        return {
            "success": False,
            "error": str(e),
            "predictions": []
        }


async def analyze_audio_with_hume(wav_file_path: str) -> Dict[str, Any]:
    """
    Analyze audio file with Hume AI Speech Prosody model.

    Automatically chunks audio >5 seconds into smaller segments.

    Args:
        wav_file_path: Path to the WAV audio file

    Returns:
        Dict containing emotion predictions from Hume AI
    """
    hume_api_key = os.getenv('HUME_API_KEY')
    if not hume_api_key:
        raise ValueError("HUME_API_KEY environment variable not set")

    try:
        from pydub import AudioSegment

        # Load audio to get actual duration
        audio = AudioSegment.from_wav(wav_file_path)
        duration_ms = len(audio)
        duration_seconds = duration_ms / 1000.0

        print(f"Audio duration: {duration_ms}ms ({duration_seconds:.2f} seconds)")

        # Hume WebSocket API limit: 5000ms (5 seconds)
        MAX_CHUNK_MS = 4500  # Use 4.5s to leave safety margin

        # If audio is within limit, send as-is
        if duration_ms <= 5000:
            print(f"âœ“ Audio is within 5s limit, analyzing directly")
            return await _analyze_single_audio(wav_file_path, hume_api_key)

        # Audio is too long, need to chunk
        print(f"âš ï¸  Audio is {duration_seconds:.1f}s, chunking into 4.5s segments...")

        chunks_data = []
        chunk_files = []

        try:
            # Split audio into chunks
            num_chunks = (duration_ms + MAX_CHUNK_MS - 1) // MAX_CHUNK_MS  # Ceiling division
            print(f"   Splitting into {num_chunks} chunks")

            for i in range(0, duration_ms, MAX_CHUNK_MS):
                start_ms = i
                end_ms = min(i + MAX_CHUNK_MS, duration_ms)

                chunk = audio[start_ms:end_ms]
                chunk_duration = len(chunk)

                # Save chunk to temporary file
                chunk_path = f"{wav_file_path}.chunk{i // MAX_CHUNK_MS}.wav"
                chunk.export(chunk_path, format="wav")
                chunk_files.append(chunk_path)

                print(f"   Chunk {i // MAX_CHUNK_MS + 1}/{num_chunks}: {start_ms}ms-{end_ms}ms ({chunk_duration}ms)")

                # Analyze chunk
                chunk_result = await _analyze_single_audio(chunk_path, hume_api_key)

                if chunk_result.get('success'):
                    # Adjust time offsets for each prediction
                    for pred in chunk_result['predictions']:
                        pred['time']['begin'] = pred['time']['begin'] + (start_ms / 1000.0) if pred['time']['begin'] else None
                        pred['time']['end'] = pred['time']['end'] + (start_ms / 1000.0) if pred['time']['end'] else None
                        pred['chunk_index'] = i // MAX_CHUNK_MS

                    chunks_data.extend(chunk_result['predictions'])
                else:
                    print(f"   âš ï¸  Chunk {i // MAX_CHUNK_MS + 1} analysis failed: {chunk_result.get('error')}")

            # Combine results from all chunks
            if chunks_data:
                return {
                    "success": True,
                    "predictions": chunks_data,
                    "total_predictions": len(chunks_data),
                    "total_duration_seconds": duration_seconds,
                    "num_chunks": num_chunks,
                    "chunked": True
                }
            else:
                return {
                    "success": False,
                    "error": "All chunks failed to analyze",
                    "predictions": [],
                    "debug_info": {
                        "num_chunks": num_chunks,
                        "total_duration_seconds": duration_seconds
                    }
                }

        finally:
            # Clean up chunk files
            for chunk_file in chunk_files:
                try:
                    if Path(chunk_file).exists():
                        Path(chunk_file).unlink()
                except Exception as e:
                    print(f"Warning: Could not delete chunk file {chunk_file}: {e}")

    except Exception as e:
        print(f"Error analyzing audio with Hume: {e}")
        import traceback
        traceback.print_exc()
        return {
            "success": False,
            "error": str(e),
            "predictions": []
        }


async def _analyze_single_audio(wav_file_path: str, hume_api_key: str) -> Dict[str, Any]:
